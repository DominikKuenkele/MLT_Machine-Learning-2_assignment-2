{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Convolutions with MIDI\n",
    "\n",
    "In this assignment, you're going to play around with the MIDI notebook we've been building in class.\n",
    "\n",
    "The code should run on mltgpu at the time of submission, but you do not need to use the GPU for this assignment.  (You can if you want to.)\n",
    "\n",
    "When testing on your own machine, in addition to the full PyTorch stack, you'll need the mido module.  Installing the scamp module is necessary if you want to listen to anything. If using Linux, you will have to install fluidsynth.\n",
    "\n",
    "You will use the [lakh](https://colinraffel.com/projects/lmd/) MIDI corpus.  A copy will be placed in the scratch directory of mltgpu; information will be provided via Canvas announcement.\n",
    "\n",
    "This assignment is due on November 1, 2022, at 23:59.  There are **25 points** and **29 bonus points** (!!!) available on this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import mido\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mido import MidiFile, KeySignatureError, Message\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 -- improve data handling and representation (4 points)\n",
    "\n",
    "Here you will take the `MessageSequence` we created in class and make the following improvements:\n",
    "\n",
    "1. Change the representation so that it can accommodate start and end symbols, as appropriate for your modeling in part 2.\n",
    "\n",
    "2. Allow for the loading of multiple channels (2 or more, possibly randomly selected), with a reasonable cutoff.  To make things simple, you can make the very wrong assumption that every note is of the same duration and therefore aligned one-by-one, and you can thus ignore duration and offset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83eb741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn.functional as functional\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class MIDITrackError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Note:\n",
    "    note: int\n",
    "    timestamp: int\n",
    "    offset: int\n",
    "    duration: int\n",
    "\n",
    "\n",
    "class MessageSequence:\n",
    "    START_NOTE = Note(128, 0, 0, 0)\n",
    "    END_NOTE = Note(129, 0, 0, 0)\n",
    "    PADDING_NOTE = Note(130, 130, 130, 130)\n",
    "    NOTE_DB = functional.one_hot(torch.arange(0, 128 + 3))\n",
    "\n",
    "    def __init__(self, mid: MidiFile, number_of_channels: int = 1, random_channels: bool = True):\n",
    "        messages = self._collect_notes(mid)\n",
    "\n",
    "        if len(messages) < number_of_channels:\n",
    "            raise MIDITrackError(f'File has only {len(messages)} channels')\n",
    "\n",
    "        if random_channels:\n",
    "            messages = {key: messages[key] for key in random.sample(list(messages.keys()), k=number_of_channels)}\n",
    "        else:\n",
    "            messages = {key: messages[key] for key in list(messages.keys())[:number_of_channels]}\n",
    "\n",
    "        self.sequences = self._calculate_note_durations(messages)\n",
    "\n",
    "    def _collect_notes(self, mid):\n",
    "        messages: defaultdict[list[Message]] = defaultdict(list)\n",
    "        try:\n",
    "            merged_track = mido.merge_tracks(mid.tracks)\n",
    "\n",
    "            for i in merged_track:\n",
    "                if i.type in ['note_on', 'note_off']:\n",
    "                    messages[i.channel].append(i)\n",
    "        except IndexError as e:\n",
    "            raise MIDITrackError from e\n",
    "        except TypeError as e:\n",
    "            raise MIDITrackError from e\n",
    "\n",
    "        # delete channels with unequal number of note_on and note_off\n",
    "        for channel in list(messages.keys()):\n",
    "            note_ons = 0\n",
    "            note_offs = 0\n",
    "            for note in messages[channel]:\n",
    "                if note.type == 'note_on':\n",
    "                    note_ons += 1\n",
    "                elif note.type == 'note_off':\n",
    "                    note_offs += 1\n",
    "\n",
    "            if note_offs != note_ons:\n",
    "                del messages[channel]\n",
    "        return messages\n",
    "\n",
    "    def _calculate_note_durations(self, messages):\n",
    "        lengths = list(sorted([(channel, len(messages[channel])) for channel in messages], key=lambda x: x[1]))\n",
    "        max_length_channel, max_length = lengths[-1]\n",
    "\n",
    "        sequences = {}\n",
    "        timecounter = 0\n",
    "        notedict = {}\n",
    "        real_sequence = []\n",
    "        for message in messages[max_length_channel]:\n",
    "            timecounter += message.time\n",
    "            if message.type == \"note_on\":\n",
    "                notedict[message.note] = timecounter\n",
    "\n",
    "            if message.type == \"note_off\":\n",
    "                if message.note not in notedict:\n",
    "                    raise MIDITrackError(f'Note {message.note} turned off before turned on')\n",
    "\n",
    "                duration = timecounter - notedict[message.note]\n",
    "                real_sequence.append(Note(\n",
    "                    note=message.note,\n",
    "                    timestamp=notedict[message.note],\n",
    "                    offset=message.time,\n",
    "                    duration=duration\n",
    "                ))\n",
    "\n",
    "        real_sequence.insert(0, self.START_NOTE)\n",
    "        real_sequence.append(self.END_NOTE)\n",
    "        sequences[max_length_channel] = real_sequence\n",
    "\n",
    "        for channel in messages:\n",
    "            if channel == max_length_channel:\n",
    "                continue\n",
    "\n",
    "            timecounter = 0\n",
    "            notedict = {}\n",
    "            real_sequence = []\n",
    "            for index, message in enumerate(list(filter(lambda x: x.type == 'note_on', messages[channel]))):\n",
    "                reference_message = sequences[max_length_channel][index + 1]\n",
    "\n",
    "                real_sequence.append(Note(\n",
    "                    note=message.note,\n",
    "                    timestamp=reference_message.timestamp,\n",
    "                    offset=reference_message.offset,\n",
    "                    duration=reference_message.duration,\n",
    "                ))\n",
    "\n",
    "            real_sequence.extend([self.PADDING_NOTE] * (int(max_length / 2) - len(real_sequence)))\n",
    "            real_sequence.insert(0, self.START_NOTE)\n",
    "            real_sequence.append(self.END_NOTE)\n",
    "            sequences[channel] = real_sequence\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    def midi_reencode(self):\n",
    "        reencoded = []\n",
    "        for channel in self.sequences:\n",
    "            active_notes = {}\n",
    "            timecounter = 0\n",
    "            for note in self.sequences[channel]:\n",
    "                note_order = sorted(active_notes.keys(), key=lambda x: active_notes[x][0])  # timestamp is tuple item 0\n",
    "                for active_note in note_order:\n",
    "                    if active_notes[active_note][0] < note.timestamp:\n",
    "                        reencoded.append(mido.Message(\"note_off\",\n",
    "                                                      channel=channel,\n",
    "                                                      note=active_note,\n",
    "                                                      velocity=95,\n",
    "                                                      time=active_notes[active_note][1]))\n",
    "                        timecounter += active_notes[active_note][1]\n",
    "                        del active_notes[active_note]\n",
    "                reencoded.append(mido.Message(\"note_on\",\n",
    "                                              channel=channel,\n",
    "                                              note=note.note,\n",
    "                                              velocity=95,\n",
    "                                              time=note.timestamp-timecounter))\n",
    "                active_notes[note] = (note.timestamp+note.duration, note.offset, note.duration)\n",
    "                timecounter = note.timestamp\n",
    "\n",
    "            note_order = sorted(active_notes.keys(), key=lambda x: active_notes[x][0])  # timestamp is tuple item 0\n",
    "            for active_note in note_order:\n",
    "                reencoded.append(mido.Message(\"note_off\",\n",
    "                                              channel=channel,\n",
    "                                              note=active_note,\n",
    "                                              velocity=95,\n",
    "                                              time=active_notes[active_note][1]))\n",
    "                del active_notes[active_note]\n",
    "\n",
    "        return reencoded\n",
    "\n",
    "    def vector_encode(self):\n",
    "        channels = list(self.sequences.values())\n",
    "        encoded = [[] for _ in range(len(channels[0]))]\n",
    "        for channel in channels:\n",
    "            for note_index, note in enumerate(channel):\n",
    "                encoded[note_index].append(self.encode_note(note))\n",
    "        return encoded\n",
    "\n",
    "    def encode_note(self, note):\n",
    "        offset = note.offset\n",
    "        duration = note.duration\n",
    "        note_value = note.note\n",
    "\n",
    "        # note_vec = self.NOTE_DB[note.note].clone().detach()\n",
    "\n",
    "        if offset > 100:\n",
    "            offset = 100\n",
    "        if duration > 4000:\n",
    "            duration = 4000\n",
    "\n",
    "        offset = offset/100\n",
    "        duration = duration/4000\n",
    "        note_value = note_value/131\n",
    "\n",
    "        return {\n",
    "            'note': torch.Tensor([note.note]),\n",
    "            'note_vector': torch.Tensor([note_value]),\n",
    "            'offset': torch.Tensor([offset]),\n",
    "            'duration': torch.Tensor([duration])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab22bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = MidiFile('data/lmd_full/8/834d6b48ccaaa7077f18c566202dbf8f.mid')\n",
    "seq = MessageSequence(file, 4, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe your changes and any special motivations for them here (in notebook Markdown):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that tracks in the MIDI files contained sometimes multiple channels, while some channels were spread over multiple tracks. To simplify, I merged all tracks into one and extracted the channels. During the extraction of the channels, I also excluded files that were not fitting the simplifications or the model (e.g. files with no 'note_off' or different numbers of 'note_on' and 'note_off').\n",
    "\n",
    "In the next step, I identified the channel with the most notes, and then aligned the other channels along this reference channel. The durations and offsets are therefore based on the reference channel and the same for all channels. This, of course is a simplification and breaks the music. The missing notes in the end of a shorter channel are filled with PADDING_NOTES. After that, all channels are prefixed with a START_NOTE and suffixed with and END_NOTE.\n",
    "\n",
    "For the vector encoding, I decided to not use a one-hot encoding for the notes, but rely on a single float as for the offset and the duration. The reason for that is that I wanted to use these three values as channels in the CNN layer. Therefore, they needed to have the same size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Convolutional Model (8 points)\n",
    "\n",
    "Replace the model below with a model with the following characteristics:\n",
    "\n",
    "1. It should include an ensemble of parallel 1D-convolutional layers (2 or more)\n",
    "2. The layers should combine into a single output representation.\n",
    "3. The layers should be different (have different kernels, windows, or strides).\n",
    "4. The layers should be able to handle multiple channels. \n",
    "5. The input will be the song representation up to time step n, and the output will be a representation of notes for a single time step across the channels at n+1. (This means that an instance will be prediction of the next note, and a song will have to be run n times to predict n characters.)\n",
    "\n",
    "Training the model will take longer than the n-gram model, especially if you're not using the GPU.\n",
    "\n",
    "You have a free hand in all other aspects of the model, as long as you explain any significant design decisions (i.e., not every minor choice, but ones with real design impact).\n",
    "\n",
    "(A bit of advice: the biggest problem here will be keeping the matrix/tensor dimensions straight...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn.functional import pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDIModel(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.song_length = 4000\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(3, 3, kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv1d(3, 3, kernel_size=4, stride=1)\n",
    "        self.conv3 = nn.Conv1d(3, 3, kernel_size=10, stride=1)\n",
    "        self.conv4 = nn.Conv1d(3, 3, kernel_size=10, stride=10)\n",
    "\n",
    "        combined_length = 12387\n",
    "\n",
    "        self.linear_sequential = nn.Sequential(\n",
    "            nn.Linear(combined_length, int(combined_length / 2)),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(int(combined_length / 2), int(combined_length / 4)),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(int(combined_length / 4), int(combined_length / 8)),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.linear_notes = nn.Linear(int(combined_length / 8), 131)\n",
    "        self.linear_offset = nn.Linear(int(combined_length / 8), 1)\n",
    "        self.linear_duration = nn.Linear(int(combined_length / 8), 1)\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=0)\n",
    "        self.sigmoid_offset = nn.Sigmoid()\n",
    "        self.sigmoid_duration = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Tensor (batch, CNN channel, MIDI channel, song length) -> (MIDI channel, batch, CNN channel, song length)\n",
    "        by_midi_channel = data.permute(2, 0, 1, 3)\n",
    "        padded = pad(by_midi_channel, (0, self.song_length - by_midi_channel.size(3)), value=130)\n",
    "\n",
    "        note_outputs = []\n",
    "        offset_outputs = []\n",
    "        duration_outputs = []\n",
    "        for channel in padded:\n",
    "            conv1 = self.dropout(self.conv1(channel))\n",
    "            conv2 = self.dropout(self.conv2(channel))\n",
    "            conv3 = self.dropout(self.conv3(channel))\n",
    "            conv4 = self.dropout(self.conv4(channel))\n",
    "\n",
    "            combined = torch.cat((conv1, conv2, conv3, conv4), dim=2)\n",
    "\n",
    "            linear = self.linear_sequential(combined)\n",
    "\n",
    "            note_output = self.linear_notes(linear[:, 0])\n",
    "            offset_output = self.linear_offset(linear[:, 1])\n",
    "            duration_output = self.linear_duration(linear[:, 2])\n",
    "\n",
    "            note_outputs.append(self.logsoftmax(note_output))\n",
    "            offset_outputs.append(self.sigmoid_offset(offset_output))\n",
    "            duration_outputs.append(self.sigmoid_duration(duration_output))\n",
    "\n",
    "        return (\n",
    "            torch.stack(note_outputs).permute(1, 0, 2),\n",
    "            torch.stack(offset_outputs).permute(1, 0, 2),\n",
    "            torch.stack(duration_outputs).permute(1, 0, 2)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain your design choices below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned above, the goal was, to use note_value, offset and duration as channels for the CNN layer. For this reason, all samples fed to the Model needed to have the same length. This length, I defined as maximum 4000 notes in a song. All samples are padded to this length.\n",
    "\n",
    "After that I predict notes for each MIDI channel separately. Each channel is run through 4 convolutional layers. Two of these should find smaller patterns with a small kernel size and the other two bigger patterns. The outputs are then concateneated and run through multiple linear layers to reduce the dimensions. Finally, the note value channel is reduced to 131 (notes + special tokens) dimensions with a final logarithmic softmax function. With this, it is ensured that only 'legal' not values can be predicted. The offset and duration channel are reduced to a single number, since they can be calculated back to a real offset/duration.\n",
    "\n",
    "I applied dropout to all layers, to reduce overfitting to my dataset. For lack of memory, I could only use a very small dataset, which is why the model does not see very varied data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Dataset sampling (4 points)\n",
    "\n",
    "Consider how the model is designed above and design a dataset generator capable of producing sample prefixes and next-characters for each time step for each song.  You can replace all the code from the original MIDI notebook with whatever you want.  Consider that there are more and less efficient ways of doing this, and that it may also be worth seeing if it's easier to do this in iterator mode where you can select random prefixes from random songs at each iteration.  You can even choose not to use the torch Dataset class at all, though it means you will have to rewrite the training loop not to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    # (L, (note_vector: (128 + 3) + offset: 1 + duration: 1) = 133)\n",
    "    song_beginning: torch.Tensor\n",
    "    note: torch.Tensor\n",
    "    offset: torch.Tensor\n",
    "    duration: torch.Tensor\n",
    "\n",
    "\n",
    "def generate_samples_per_song(song: MessageSequence) -> list[Sample]:\n",
    "    vectors = song.vector_encode()\n",
    "\n",
    "    number_midi_channels = len(vectors[0])\n",
    "    number_cnn_channels = 3\n",
    "    length_song = len(vectors)\n",
    "\n",
    "    padding_note = song.encode_note(song.PADDING_NOTE)\n",
    "    samples: list[Sample] = []\n",
    "    for i in range(1, length_song):\n",
    "        song_beginning: list[list[torch.Tensor]] = [[[] for _ in range(number_midi_channels)] for _ in range(number_cnn_channels)]\n",
    "        for note in vectors[0:i]:\n",
    "            for channel_index, channel_note in enumerate(note):\n",
    "                song_beginning[0][channel_index].append(channel_note['note_vector'])\n",
    "                song_beginning[1][channel_index].append(channel_note['offset'])\n",
    "                song_beginning[2][channel_index].append(channel_note['duration'])\n",
    "        \n",
    "        # pad song_beginning\n",
    "        for midi_channel in range(number_midi_channels):\n",
    "            song_beginning[0][midi_channel].extend([padding_note['note_vector']] * (length_song - i))\n",
    "            song_beginning[1][midi_channel].extend([padding_note['offset']] * (length_song - i))\n",
    "            song_beginning[2][midi_channel].extend([padding_note['duration']] * (length_song - i))\n",
    "\n",
    "\n",
    "        notes: list[torch.Tensor] = [[] for _ in range(number_midi_channels)]\n",
    "        offsets: list[torch.Tensor] = [[] for _ in range(number_midi_channels)]\n",
    "        durations: list[torch.Tensor] = [[] for _ in range(number_midi_channels)]\n",
    "        for channel, channel_note in enumerate(vectors[i]):\n",
    "            notes[channel] = channel_note['note']\n",
    "            offsets[channel] = channel_note['offset']\n",
    "            durations[channel] = channel_note['duration']\n",
    "\n",
    "        samples.append(Sample(\n",
    "            song_beginning=torch.stack([torch.stack([torch.cat(midi_channel) for midi_channel in cnn_channels]) for cnn_channels in song_beginning]),\n",
    "            note=torch.stack(notes),\n",
    "            offset=torch.stack(offsets),\n",
    "            duration=torch.stack(durations)\n",
    "        ))\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def generate_samples(songlist: list[MessageSequence]) -> list[Sample]:\n",
    "    samples: list[Sample] = []\n",
    "    for song in songlist:\n",
    "        samples += generate_samples_per_song(song)\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDINotesDataset(Dataset):\n",
    "    def __init__(self, mididir: str = None, index_file: str = None, maximum: int = 500) -> None:\n",
    "        if mididir is None and index_file is None:\n",
    "            raise ValueError(\"Must provide at least one of --mididir or --index_file\")\n",
    "\n",
    "        write_index = False\n",
    "\n",
    "        if mididir is not None:\n",
    "            search_pattern = mididir + '/**//*.mid'\n",
    "            file_list = glob.glob(search_pattern, recursive=True)\n",
    "            if index_file is not None:\n",
    "                write_index = True\n",
    "        else:\n",
    "            with open(index_file, 'r') as f:\n",
    "                file_list = f.read().splitlines()\n",
    "\n",
    "        mss: list[MessageSequence] = []\n",
    "        count = 0\n",
    "        successfull_files: set[str] = set()\n",
    "        for filename in file_list:\n",
    "            try:\n",
    "                midifile = MidiFile(filename)\n",
    "                ms = MessageSequence(midifile, 2)\n",
    "                successfull_files.add(filename)\n",
    "            except MIDITrackError:\n",
    "                continue\n",
    "            except EOFError:\n",
    "                continue\n",
    "            except KeySignatureError:\n",
    "                continue\n",
    "            except OSError:\n",
    "                continue\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            mss.append(ms)\n",
    "\n",
    "            count += 1\n",
    "            if count == maximum:\n",
    "                break\n",
    "\n",
    "        if write_index:\n",
    "            with open(index_file, 'w') as f:\n",
    "                f.write('\\n'.join(successfull_files))\n",
    "        self.notes = generate_samples(mss)\n",
    "\n",
    "    def __getitem__(self, i) -> Sample:\n",
    "        return self.notes[i]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ca48cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_files = 100\n",
    "full_dataset = MIDINotesDataset(index_file='index_lmd_matched.txt', maximum=maximum_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d68497aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full: 65872\n",
      "Train: 52697\n",
      "Test: 13175\n"
     ]
    }
   ],
   "source": [
    "split = int(0.8 * len(full_dataset))\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [split, len(full_dataset) - split])\n",
    "print(f'Full: {len(full_dataset)}')\n",
    "print(f'Train: {len(train_dataset)}')\n",
    "print(f'Test: {len(test_dataset)}')\n",
    "\n",
    "del full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0dd016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'saved/train_dataset_{maximum_files}', 'wb') as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "\n",
    "with open(f'saved/test_dataset_{maximum_files}', 'wb') as f:\n",
    "    pickle.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'saved/train_dataset_100', 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "with open(f'saved/test_dataset_100', 'rb') as f:\n",
    "    pickle.load(f)\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "030ef44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4813])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1].song_beginning.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe any significant choices you made in designing the mode of access to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest change I made was the representation of the sample. Each sample contains of a beginning of the song until the specific note and the representation of the note itself (note_value, offset, duration). Also the representation of the song beginning is a tensor of multiple note representations. Since I wanted to combine everything in one tensor, all song beginnings needed to have the same size. Therfore, I padded them with padding notes on the right side. The song beginning has the shape (CNN channel, MIDI channel, song length) where the CNN channels are note_value, offset and duration. With this representation, I get very big tensors that occupy a lot of memory. That's why, I was able to only use a very small dataset of 100 songs which produced around 76000 samples in total.\n",
    "\n",
    "Furthermore, I implemented a solution, to index processable midi files from the dataset, to reload them faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Training loop (2 points)\n",
    "\n",
    "Adapt the training loop to the way you organized access to the dataset and to the model you wrote.  Make any other improvements, such as trying out a different optimizer.  Make sure it is possible to vary the batch size as well as the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca89eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    song_beginnings = []\n",
    "    notes = []\n",
    "    offsets = []\n",
    "    durations = []\n",
    "\n",
    "    max_length = 0\n",
    "    for sample in data:\n",
    "        song_beginnings.append(sample.song_beginning)\n",
    "        max_length = max(max_length, sample.song_beginning.size(2))\n",
    "\n",
    "        notes.append(sample.note)\n",
    "        offsets.append(sample.offset)\n",
    "        durations.append(sample.duration)\n",
    "    \n",
    "    padded_song_beginnings = [pad(song_beginning, (0, max_length - song_beginning.size(2)), value=130) for song_beginning in song_beginnings]\n",
    "\n",
    "    return {\n",
    "        'song_beginnings': torch.stack(padded_song_beginnings),\n",
    "        'notes': torch.stack(notes),\n",
    "        'offsets': torch.stack(offsets),\n",
    "        'durations': torch.stack(durations)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def train(dataloader, epochs=10, device=torch.device('cpu'), dropout_prob=0.2, learning_rate=0.001):\n",
    "    mm = MIDIModel(dropout_prob).to(device)\n",
    "    optimizer = optim.Adam(mm.parameters(), lr=learning_rate)\n",
    "    note_criterion = nn.NLLLoss()\n",
    "    offset_citerion = nn.L1Loss()\n",
    "    duration_criterion = nn.L1Loss()\n",
    "    print(f'{epochs} EPOCHS - {math.floor(len(dataloader.dataset) / dataloader.batch_size)} BATCHES PER EPOCH')\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            song_beginnings = batch['song_beginnings'].to(device)\n",
    "            notes = batch['notes'].type(torch.LongTensor).to(device)\n",
    "            offsets = batch['offsets'].to(device)\n",
    "            durations = batch['durations'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            (note_output, offset_output, duration_output) = mm(song_beginnings)\n",
    "            note_loss = torch.exp(-note_criterion(note_output.permute(0, 2, 1), notes.squeeze()))\n",
    "            offset_loss = offset_citerion(offset_output, offsets)\n",
    "            duration_loss = duration_criterion(duration_output, durations)\n",
    "\n",
    "            loss = note_loss + offset_loss + duration_loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sys.stdout.write(f'\\repoch {epoch}, batch {i}: {round(total_loss / (i + 1), 4)}')\n",
    "\n",
    "        print()\n",
    "    return mm.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4796062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "# model = train(dataloader, 10, torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there are any remarks you have on the training loop, put them here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5792c",
   "metadata": {},
   "source": [
    "For the training, I switched to the Adam optimizer, which produced much better results. Aditionally, I used the L1-Loss for offset and duration, to deal easily with multiple MIDI channels. Furthermore, I changed some code, to get better logs and see, how the loss is changing during training. This didn't have an effect on the training itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Evaluation (7 points)\n",
    "\n",
    "Actually predicting accuracy of note prediction in a set of songs is probably unlikely to work.  So instead we will calculate the perplexity of your model under different training assumptions (for example, epochs, dropout probability -- if you used dropout -- and/or hidden layer size).  Divide your dataset into training and validation sets and use the validation for the perplexity calculation.  (Note that you are predicting notes across multiple channels, so will have to combine perplexities across the channels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0.2, 0.0001)\n",
      "5 EPOCHS - 411 BATCHES PER EPOCH\n",
      "epoch 0, batch 410: 0.4271\n",
      "epoch 1, batch 410: 0.4001\n",
      "epoch 2, batch 410: 0.3993\n",
      "epoch 3, batch 410: 0.3998\n",
      "epoch 4, batch 410: 0.3994\n",
      "(5, 0.3, 0.0001)\n",
      "5 EPOCHS - 411 BATCHES PER EPOCH\n",
      "epoch 0, batch 410: 0.4354\n",
      "epoch 1, batch 410: 0.4011\n",
      "epoch 2, batch 410: 0.3995\n",
      "epoch 3, batch 410: 0.3987\n",
      "epoch 4, batch 410: 0.3982\n",
      "(5, 0.2, 1e-05)\n",
      "5 EPOCHS - 411 BATCHES PER EPOCH\n",
      "epoch 0, batch 410: 0.4572\n",
      "epoch 1, batch 410: 0.4221\n",
      "epoch 2, batch 410: 0.3972\n",
      "epoch 3, batch 410: 0.3833\n",
      "epoch 4, batch 410: 0.3782\n",
      "(3, 0, 0.0001)\n",
      "3 EPOCHS - 411 BATCHES PER EPOCH\n",
      "epoch 0, batch 410: 0.4149\n",
      "epoch 1, batch 410: 0.3844\n",
      "epoch 2, batch 410: 0.3849\n"
     ]
    }
   ],
   "source": [
    "parameters = [\n",
    "    # (epochs, dropout, learning_rate)\n",
    "    (5, 0.2, 0.0001),\n",
    "    (5, 0.3, 0.0001),\n",
    "    (5, 0.2, 0.00001),\n",
    "    (3, 0,   0.0001),\n",
    "]\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "models = {}\n",
    "for parameter in parameters:\n",
    "    print(parameter)\n",
    "    models[parameter] = train(dataloader, parameter[0], torch.device('cuda:0'), parameter[1], parameter[2])  \n",
    "    models[parameter].eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b5c02b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0.2, 0.0001) 131.1883544921875\n",
      "(5, 0.3, 0.0001) 131.12937927246094\n",
      "(5, 0.2, 1e-05) 131.22671508789062\n",
      "(3, 0, 0.0001) 131.2026824951172\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "with torch.no_grad():\n",
    "    for parameter, model in models.items():\n",
    "        model.to(device)\n",
    "        losses = []\n",
    "        for batch in DataLoader(test_dataset, batch_size=128, drop_last=True, collate_fn=custom_collate):\n",
    "            song_beginnings = batch['song_beginnings'].to(device)\n",
    "            notes = batch['notes'].type(torch.LongTensor).to(device)\n",
    "\n",
    "            (note_output, _, _) = model(song_beginnings)\n",
    "            note_loss = loss(torch.exp(note_output).permute(0, 2, 1), notes.squeeze())\n",
    "            \n",
    "            losses.append(note_loss)\n",
    "        average = torch.mean(torch.stack(losses))\n",
    "        perplexity = torch.exp(average).item()\n",
    "        print(parameter, perplexity)    \n",
    "\n",
    "        model.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your remarks on your evaluation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset occupied a lot of memory, so I could train the model with only a very small set of songs (100 songs). Even though they produced around 76.000 samples, it is not much data to learn from. The data for example learns only 100 times per epoch, how to predict the start of a song. The same applies also to the GPU memory, and GPU time which is why I run the model with a maximum of 5 epochs. For that reason and the fact that the data is very simplified, I didn't expect very good results. Anyway, the loss decreased all the time, which indicated that the model is learning despite these limitations.\n",
    "\n",
    "During my training, the model mostly only improved in the first epoch. All further epochs give only a slight better loss. Changing the learning rate from 0.001 to 0.0001 also improved the learning process. As seen in model 3, an even smaller learning rate worsened the model in the first epochs a little, but with more epochs it, lowered the loss the most. \n",
    "\n",
    "When we look at the perplexities, it can be seen that they are very high while there is not much difference between the models. The best model here is the model with a higher dropout (0,3) and a lower learning rate (0.001). The difference is so small that this also is a result of different randomized datasets or initilizations. \n",
    "\n",
    "There may be a few reasons for these performance. As mentioned, I think a less simplified and bigger dataset might improve the model a lot. With the current limited data, it is probably diffcult for the model to find recurrent patterns in all songs. Futhermore, the big amount of padding is maybe also a limiting factor. There is padding to pad all midi channels to the same size, to make the batches the same size and to feed the same length to the CNNs. The actual data then only is a very small part. For this, it may be an idea, to decrease it, by for example using a different method to align channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part 1 -- \"Music\" (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to properly install [scamp](http://scamp.marcevanstein.com/) to do this bonus. You can rewrite the mode of song generation here to take into account your convolutional process.  Then use scamp to play the (multi-channel/simultaneous note music back).  Try to see if you get any quality improvement at all by using better parameters. (It will probably sound awful no matter what.)  If you want to train on mltgpu and play music on your own computer, you'll have to also write a way to save and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "# This is just to get the first two notes out of the development song.\n",
    "vecs = x.vector_encode()\n",
    "\n",
    "def generate_music(model, note1, note2, length=30, diversity=5):\n",
    "    note_db = functional.one_hot(torch.arange(0, 128))\n",
    "    newsong = [note1, note2]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "            notepair = torch.cat((note1, note2))\n",
    "            fake_batch = torch.stack([notepair] + [torch.randn(260) for _ in range(24)])\n",
    "            (note_output, offset_output, duration_output) = model(fake_batch)\n",
    "            note_output = note_output[0]\n",
    "            offset_output = offset_output[0]\n",
    "            duration_output = duration_output[0]\n",
    "            print(\"note_output: {}\".format(note_output))\n",
    "            notesort = torch.argsort(note_output, descending=True)\n",
    "            print(\"notesort: {}\".format(notesort))\n",
    "            noteset = notesort[:diversity]\n",
    "            print(\"noteset: {}\".format(noteset))\n",
    "            notenum = int(choice(noteset.numpy()))\n",
    "            print(\"notenum: {}\".format(notenum))\n",
    "            note1 = note2\n",
    "            print(\"testgen {} {} {}\".format(note_db[notenum].clone().detach(), offset_output, duration_output))\n",
    "            note2 = torch.cat((note_db[notenum].clone().detach(), torch.Tensor([offset_output]), \n",
    "                                                                               torch.Tensor([duration_output])))\n",
    "            newsong.append(note2)\n",
    "    return newsong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconvert_song(notetensors):\n",
    "    return [(int(torch.argmax(x[0:128])), int(torch.floor(x[128] * 100)), int(torch.floor(x[129] * 4000))) for x in notetensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_back(model_ouptut, starting_time):\n",
    "    sequence = []\n",
    "    for (note, offset, duration) in model_ouptut:\n",
    "        sequence.append((note, starting_time, offset, duration))\n",
    "        starting_time += duration - offset\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scamp import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = Session().run_as_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using preset Clarinet for clarinet\n"
     ]
    }
   ],
   "source": [
    "clarinet = sess.new_part(\"clarinet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in converted_song:\n",
    "    clarinet.play_note(n[0], 0.8, n[2]/1000)\n",
    "    time.sleep(n[2]/1000 + 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your remarks on the quality of the music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part 2 - 2D-convolutions (6 points)\n",
    "\n",
    "Define a model as in part 2 that restructures your representation as an ensemble of 2D convolutional models (using the additional dimension to handle multiple MIDI channels).  This will probably require that you rebuild other parts of the pipeline to accommodate it.\n",
    "\n",
    "Do an evaluation of the output in terms of perplexity (and, optionally, musical quality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your code here (in as many cells as you need):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your remarks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part 3 - Durations (20 points)\n",
    "\n",
    "Starting from the song representation, find a way to properly handle durations across multiple channels so that your code is not reliant on an incorrect alignment of the sequence of notes.  Evaluate as in Bonus Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your remarks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit a filled-out version of this notebook via Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('lt2326-a2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c009f082b66f9c9ab96d6f41981f37dc2a56e2840fe763ce6f4607959f42c89b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
